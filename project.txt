
=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\docker-compose.yml ===
services:
  postgres:
    image: postgres:14-alpine
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./postgres-data:/var/lib/postgresql/data
  
  api:
    build:
      context: ./api
    ports:
      - "8000:8000"
    volumes:
      - ./api:/app
    environment:
      - PYTHONUNBUFFERED=1
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - DB_POOL_MIN=1
      - DB_POOL_MAX=20
      - RABBIT_URL=amqp://${RABBIT_USER:-guest}:${RABBIT_PASSWORD:-guest}@rabbitmq:5672/
    restart: unless-stopped
    depends_on:
      - postgres
      - rabbitmq
  rabbitmq:
    image: rabbitmq:3.11-management
    restart: unless-stopped
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBIT_USER:-guest}
      RABBITMQ_DEFAULT_PASS: ${RABBIT_PASSWORD:-guest}
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    healthcheck:
      test: ["CMD", "rabbitmqctl", "status"]
      interval: 10s
      timeout: 5s
      retries: 8
      start_period: 10s

  worker:
    build:
      context: ./api
    command: ["python", "-m", "app.services.worker"]
    volumes:
      - ./api:/app
    environment:
      - PYTHONUNBUFFERED=1
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - DB_POOL_MIN=1
      - DB_POOL_MAX=10
      - RABBIT_URL=amqp://${RABBIT_USER:-guest}:${RABBIT_PASSWORD:-guest}@rabbitmq:5672/
      - RABBIT_PREFETCH=3
    depends_on:
      - postgres
      - rabbitmq
    restart: unless-stopped
  dispatcher:
    build:
      context: ./api
    command: ["python", "-m", "app.services.dispatcher_worker"]
    volumes:
      - ./api:/app
    environment:
      - PYTHONUNBUFFERED=1
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - DB_POOL_MIN=1
      - DB_POOL_MAX=5
      - RABBIT_URL=amqp://${RABBIT_USER:-guest}:${RABBIT_PASSWORD:-guest}@rabbitmq:5672/
      - WEBHOOK_URL=${WEBHOOK_URL}
      - DISPATCH_INTERVAL=30
    depends_on:
      - postgres
      - rabbitmq
    restart: unless-stopped

volumes:
  rabbitmq-data:


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\main.py ===
from app.api import app

__all__ = ["app"]

=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\api.py ===
from fastapi import FastAPI, Depends, HTTPException
from fastapi.responses import JSONResponse
import logging
import time
from fastapi.middleware.cors import CORSMiddleware
from app.core.db import init_db, close_db
from app.core.mq import init_rabbit, close_rabbit, publish_message
from contextlib import asynccontextmanager
from fastapi import FastAPI
from app.core.db import db_manager
from app import repository
from typing import Optional
from pydantic import BaseModel
from uuid import UUID
import asyncio
import json

class CreateRequest(BaseModel):
    originalText: str
    externalId: Optional[str] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Inicializa recursos de forma centralizada
    await db_manager.connect()  
    await init_rabbit()         
    yield
    # Limpeza graciosa no desligamento
    await close_rabbit()        
    await db_manager.disconnect() 

app = FastAPI(lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

logger = logging.getLogger("processing_api")
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s: %(message)s"))
    logger.addHandler(handler)
logger.setLevel(logging.INFO)

_rabbit = None

@app.post("/processing")
async def create_processing(req: CreateRequest):
    start = time.monotonic()
    id = await repository.create_processing(req.originalText, req.externalId)
    connection = _rabbit[0]

    async def _bg_publish(conn, payload):
        try:
            await publish_message(conn, "processing_queue", payload)
        except Exception as e:
            print(f"background publish failed for id={id}: {e}")

    message_bytes = json.dumps({"id": str(id), "originalText": req.originalText}).encode()
    try:
        asyncio.create_task(_bg_publish(connection, message_bytes))
    except Exception as e:
        print(f"failed to schedule background publish for id={id}: {e}")

    logger.info(f"create_processing id={id} duration_ms={(time.monotonic()-start)*1000:.1f} returned=accepted")
    return JSONResponse(status_code=202, content={"id": str(id)})
     

@app.get("/processing/{id}")
async def get_processing(id: UUID):
    start = time.monotonic()
    model = await repository.get_processing(id)
    duration_ms = (time.monotonic() - start) * 1000
    if not model:
        logger.info(f"get_processing id={id} duration_ms={duration_ms:.1f} returned=404")
        raise HTTPException(status_code=404, detail="Not found")
    logger.info(f"get_processing id={id} duration_ms={duration_ms:.1f} status={model.shipment.status}")
    return model.model_dump()


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\models.py ===
from uuid import UUID, uuid4
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime
from enum import Enum


class RegisterProcessStatus(str, Enum):
    NOT_QUEUED = "not_queued"       # Ainda não foi para a fila
    ON_QUEUE = "on_queue"           # RabbitMQ/Kafka
    PROCESSING = "processing"       # Worker pegou
    COMPLETED = "completed"         # Sucesso -> Gatilho para ShipmentStatus.READY
    ERROR_RETRY = "error_retry"     # Voltar para fila (incrementa retry_count)
    ERROR_FATAL = "error_fatal"     # Max_retries excedido


class ShipmentStatus(str, Enum):
    NOT_READY = "not_ready"         # Ainda processando IA
    READY = "ready"                 # Pronto para o Dispatcher pegar
    SENT = "sent"                   # Webhook recebeu 200 OK
    ERROR_RETRY = "error_retry"     # Erro 500 ou timeout (Circuit Breaker atua aqui)
    ERROR_FATAL = "error_fatal"     # Erro 4xx ou max_retries excedido

class Status(str, Enum):
    RECEIVED = "received"
    PROCESSING = "processing"             # Agrega RegisterProcessStatus: ON_QUEUE, PROCESSING, RETRY
    WAITING_HUMAN_REVIEW = "waiting_human_review" # Caso a confiança da IA seja baixa ou haja erro fatal
    READY_TO_SHIP = "ready_to_ship"       # (Opcional) Momento entre o fim da IA e o início do envio
    ON_SHIPMENT = "on_shipment"           # Tentando entregar no webhook
    FINISHED = "finished"                 # Tudo concluído
    ERROR = "error"                       # Se der fatal no shipment

class Shipment(BaseModel):
    id: UUID = Field(default_factory=uuid4)
    status: ShipmentStatus = ShipmentStatus.NOT_READY
    attemptCount: int = 0

class Processing(BaseModel):
    id: UUID = Field(default_factory=uuid4)
    status: RegisterProcessStatus = RegisterProcessStatus.NOT_QUEUED
    attemptCount: int = 0

class Result(BaseModel):
    id: UUID = Field(default_factory=uuid4)
    confidence: float
    data: dict
class RegisterProcessEvent(BaseModel):
    id: UUID = Field(default_factory=uuid4)
    status: Status = Status.RECEIVED
    shipment: Shipment = Field(default_factory=Shipment)
    processing: Processing = Field(default_factory=Processing)
    externalId: Optional[str] = None
    originalText: str
    result: Optional[dict] = None
    createdAt: datetime = Field(default_factory=datetime.utcnow)
    updatedAt: datetime = Field(default_factory=datetime.utcnow)


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\repository.py ===
import json
from typing import Optional, List, Dict
from uuid import UUID
from app.models import (
    RegisterProcessEvent,
    RegisterProcessStatus,
    Status,
    ShipmentStatus,
    Shipment,
    Processing,
)
from app.core.db import db_manager


def row_to_model(row) -> Optional[RegisterProcessEvent]:
    if not row:
        return None
    shipment = None
    if row.get("shipment_id"):
        shipment = Shipment(
            id=row.get("shipment_id"),
            status=ShipmentStatus(row.get("shipment_status")),
            attemptCount=row.get("shipment_attempt_count") or 0,
        )
    else:
        shipment = Shipment()

    register_process = None
    if row.get("register_process_id"):
        register_process = Processing(
            id=row.get("register_process_id"),
            status=RegisterProcessStatus(row.get("register_status")),
            attemptCount=row.get("register_attempt_count") or 0,
        )
    else:
        register_process = Processing()

    return RegisterProcessEvent(
        id=row["id"],
        status=Status(row.get("status")) if row.get("status") is not None else Status.RECEIVED,
        shipment=shipment,
        processing=register_process,
        result=(json.loads(row.get("result")) if isinstance(row.get("result"), str) else row.get("result")),
        externalId=row.get("external_id"),
        originalText=row.get("original_text"),
        createdAt=row.get("created_at"),
        updatedAt=row.get("updated_at"),
    )


async def create_processing(original_text: str, external_id: str = None):
    async with db_manager.get_connection() as conn:
        async with conn.transaction():
            # create processing (was register_process)
            register_id = await conn.fetchval(
                "INSERT INTO processing(status, attempt_count, created_at, updated_at) VALUES($1, $2, now(), now()) RETURNING id",
                RegisterProcessStatus.NOT_QUEUED.value,
                0,
            )
            # create shipment
            shipment_id = await conn.fetchval(
                "INSERT INTO shipment(status, attempt_count, created_at, updated_at) VALUES($1, $2, now(), now()) RETURNING id",
                ShipmentStatus.NOT_READY.value,
                0,
            )
            # create processing_status
            proc_id = await conn.fetchval(
                "INSERT INTO register_process_event(original_text, external_id, status, register_process_id, shipment_id, created_at, updated_at) VALUES($1, $2, $3, $4, $5, now(), now()) RETURNING id",
                original_text,
                external_id,
                Status.RECEIVED.value,
                register_id,
                shipment_id,
            )
    return proc_id


async def get_processing(id: UUID) -> Optional[RegisterProcessEvent]:
    async with db_manager.get_connection() as conn:
        row = await conn.fetchrow(
            "SELECT p.*, r.status as register_status, r.attempt_count as register_attempt_count, r.id as register_process_id, s.status as shipment_status, s.attempt_count as shipment_attempt_count, s.id as shipment_id FROM register_process_event p LEFT JOIN processing r ON r.id = p.register_process_id LEFT JOIN shipment s ON s.id = p.shipment_id WHERE p.id = $1",
            id,
        )
    if not row:
        return None
    return row_to_model(row)


async def update_status(id: UUID, status: str):
    async with db_manager.get_connection() as conn:
        return await conn.execute("UPDATE register_process_event SET status = $1, updated_at = now() WHERE id = $2", status, id)


async def update_result(id: UUID, result: dict):
    """When worker finishes analysis: set processing_status.result, mark processing as READY_TO_SHIP,
    set register_process to COMPLETED and shipment to READY.
    """
    async with db_manager.get_connection() as conn:
        async with conn.transaction():
            row = await conn.fetchrow("SELECT register_process_id, shipment_id FROM register_process_event WHERE id = $1", id)
            if not row:
                return None
            register_id = row.get("register_process_id")
            shipment_id = row.get("shipment_id")

            await conn.execute(
                "UPDATE register_process_event SET status = $1, result = $2::jsonb, updated_at = now() WHERE id = $3",
                Status.READY_TO_SHIP.value,
                json.dumps(result, default=str),
                id,
            )

            if register_id:
                await conn.execute(
                    "UPDATE processing SET status = $1, updated_at = now() WHERE id = $2",
                        RegisterProcessStatus.COMPLETED.value,
                        register_id,
                )

            if shipment_id:
                await conn.execute(
                    "UPDATE shipment SET status = $1, updated_at = now() WHERE id = $2",
                    ShipmentStatus.READY.value,
                    shipment_id,
                )
    return True


async def fetch_pending_shipments() -> List[Dict]:
    async with db_manager.get_connection() as conn:
        rows = await conn.fetch(
            "SELECT p.id as processing_id, p.result, s.id as shipment_id, s.status as shipment_status, s.attempt_count as shipment_attempt_count FROM register_process_event p JOIN shipment s ON s.id = p.shipment_id WHERE s.status != 'sent' AND p.result IS NOT NULL"
        )
    out = []
    for r in rows:
        out.append(
            {
                "processing_id": r.get("processing_id"),
                "result": r.get("result"),
                "shipment": {"id": r.get("shipment_id"), "status": r.get("shipment_status"), "attemptCount": r.get("shipment_attempt_count")},
            }
        )
    return out


async def mark_shipment_sent(processing_id: UUID):
    async with db_manager.get_connection() as conn:
        row = await conn.fetchrow("SELECT shipment_id FROM register_process_event WHERE id = $1", processing_id)
        if not row:
            return None
        shipment_id = row.get("shipment_id")
        if shipment_id:
            return await conn.execute("UPDATE shipment SET status = $1, updated_at = now() WHERE id = $2", ShipmentStatus.SENT.value, shipment_id)
    return None


async def update_register_process_status_by_processing_id(processing_id: UUID, status: str):
    """Update register_process.status given a processing_status id."""
    async with db_manager.get_connection() as conn:
        row = await conn.fetchrow("SELECT register_process_id FROM register_process_event WHERE id = $1", processing_id)
        if not row:
            return None
        register_id = row.get("register_process_id")
        if register_id:
            return await conn.execute("UPDATE processing SET status = $1, updated_at = now() WHERE id = $2", status, register_id)
    return None


async def set_shipment_status_by_processing_id(processing_id: UUID, shipment_status: str, increment_attempt: bool = False):
    """Set shipment.status (and optionally increment attempt_count) for the shipment related to processing_status id."""
    async with db_manager.get_connection() as conn:
        async with conn.transaction():
            row = await conn.fetchrow("SELECT shipment_id FROM register_process_event WHERE id = $1", processing_id)
            if not row:
                return None
            shipment_id = row.get("shipment_id")
            if not shipment_id:
                return None
            if increment_attempt:
                return await conn.execute(
                    "UPDATE shipment SET status = $1, attempt_count = COALESCE(attempt_count,0) + 1, updated_at = now() WHERE id = $2",
                    shipment_status,
                    shipment_id,
                )
            else:
                return await conn.execute(
                    "UPDATE shipment SET status = $1, updated_at = now() WHERE id = $2",
                    shipment_status,
                    shipment_id,
                )


async def get_shipment_attempts_by_processing_id(processing_id: UUID):
    async with db_manager.get_connection() as conn:
        row = await conn.fetchrow("SELECT s.attempt_count as shipment_attempt_count FROM register_process_event p JOIN shipment s ON s.id = p.shipment_id WHERE p.id = $1", processing_id)
        if not row:
            return None
        return row.get("shipment_attempt_count")

=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\__init__.py ===
"""Application package"""


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\core\db.py ===
import os
import logging
import asyncpg
from typing import Optional
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)

class DatabaseManager:
    _pool: Optional[asyncpg.Pool] = None

    @classmethod
    async def connect(cls):
        """Inicializa a pool global se não existir."""
        if cls._pool is None:
            dsn = os.getenv("DATABASE_URL")
            try:
                cls._pool = await asyncpg.create_pool(
                    dsn,
                    min_size=int(os.getenv("DB_POOL_MIN", "5")),
                    max_size=int(os.getenv("DB_POOL_MAX", "20")),
                    command_timeout=60.0
                )
                logger.info("Database pool connection established.")
            except Exception as e:
                logger.error(f"Could not connect to database: {e}")
                raise

    @classmethod
    async def disconnect(cls):
        """Fecha a pool graciosamente."""
        if cls._pool:
            await cls._pool.close()
            cls._pool = None
            logger.info("Database pool connection closed.")

    @classmethod
    @asynccontextmanager
    async def get_connection(cls):
        """Context manager para adquirir e liberar conexões de forma segura."""
        if cls._pool is None:
            await cls.connect()
        
        async with cls._pool.acquire() as connection:
            yield connection

db_manager = DatabaseManager()

=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\core\db_service.py ===
from typing import Optional
import json
from app.core.db import init_db, get_pool


class DBService:
    """Centralized DB access wrapper around asyncpg pool.

    Use `await DBService.get()` to obtain the singleton instance.
    This keeps all raw DB interactions in one place.
    """

    _instance: Optional["DBService"] = None

    def __init__(self, pool):
        self.pool = pool

    @classmethod
    async def get(cls) -> "DBService":
        if cls._instance is None:
            await init_db()
            pool = get_pool()
            if pool is None:
                raise RuntimeError("DB pool not initialized")
            cls._instance = DBService(pool)
        return cls._instance

    async def fetch(self, sql: str, *args):
        async with self.pool.acquire() as conn:
            return await conn.fetch(sql, *args)

    async def fetchrow(self, sql: str, *args):
        async with self.pool.acquire() as conn:
            return await conn.fetchrow(sql, *args)

    async def fetchval(self, sql: str, *args):
        async with self.pool.acquire() as conn:
            return await conn.fetchval(sql, *args)

    async def execute(self, sql: str, *args):
        async with self.pool.acquire() as conn:
            return await conn.execute(sql, *args)

    def transaction(self):
        # Return pool-level transaction context manager
        return self.pool.transaction()


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\core\mq.py ===
import asyncio
import os
from typing import Optional, Tuple
from dotenv import load_dotenv
import aio_pika
from urllib.parse import urlparse

load_dotenv()

RABBIT_URL = os.getenv("RABBIT_URL", "amqp://guest:guest@rabbitmq:5672/")

_connection: Optional[aio_pika.RobustConnection] = None
_channel: Optional[aio_pika.RobustChannel] = None
_queue: Optional[aio_pika.Queue] = None


async def init_rabbit(retries: int = 15, delay: float = 1.0) -> Tuple[aio_pika.RobustConnection, aio_pika.RobustChannel, aio_pika.Queue]:
    global _connection, _channel, _queue
    if _connection is not None:
        return _connection, _channel, _queue
    last_exc = None
    for attempt in range(retries):
        conn = None
        try:
            print(f"attempting to connect to rabbitmq (attempt {attempt+1}/{retries})")
            parsed = urlparse(RABBIT_URL)
            host = parsed.hostname or "rabbitmq"
            port = parsed.port or 5672
            try:
                fut = asyncio.open_connection(host, port)
                reader, writer = await asyncio.wait_for(fut, timeout=3.0)
                writer.close()
                try:
                    await writer.wait_closed()
                except Exception:
                    pass
            except Exception:
                raise ConnectionError(f"tcp connect to {host}:{port} failed")

            conn = await aio_pika.connect_robust(RABBIT_URL)
            chan = await conn.channel()

            try:
                prefetch = int(os.getenv("RABBIT_PREFETCH"))
                await chan.set_qos(prefetch_count=prefetch)
            except Exception:
                pass
            q = await chan.declare_queue("processing_queue", durable=True)
            _connection = conn
            _channel = chan
            _queue = q
            return _connection, _channel, _queue
        except Exception as e:
            last_exc = e
            if conn is not None:
                try:
                    await conn.close()
                except Exception:
                    try:
                        loop = getattr(conn, "_loop", None)
                        if loop is not None and loop.is_running():
                            asyncio.run_coroutine_threadsafe(conn.close(), loop)
                    except Exception:
                        pass
            await asyncio.sleep(delay * (1 + attempt))
    raise last_exc


async def close_rabbit(connection: Optional[aio_pika.RobustConnection] = None):
    global _connection, _channel, _queue
    conn = connection or _connection
    if conn:
        try:
            await conn.close()
        except RuntimeError:
            loop = getattr(conn, "_loop", None)
            if loop is not None and loop.is_running():
                try:
                    asyncio.run_coroutine_threadsafe(conn.close(), loop)
                except Exception:
                    pass
            else:
                try:
                    conn.close()
                except Exception:
                    pass
    _connection = None
    _channel = None
    _queue = None


async def publish_message(connection_or_channel, routing_key: str, message_bytes: bytes):
    """Publish a message. Accepts either a connection or an already-open channel.

    To be safe under high concurrency, prefer passing the connection object; a
    fresh channel will be opened per publish and closed afterwards.
    """
    try:
        default_exchange = getattr(connection_or_channel, "default_exchange", None)
    except Exception:
        default_exchange = None

    if default_exchange is not None:
        await connection_or_channel.default_exchange.publish(
            aio_pika.Message(body=message_bytes, delivery_mode=aio_pika.DeliveryMode.PERSISTENT),
            routing_key=routing_key,
        )
        return

    conn = connection_or_channel
    chan = None
    try:
        chan = await conn.channel()
        await chan.default_exchange.publish(
            aio_pika.Message(body=message_bytes, delivery_mode=aio_pika.DeliveryMode.PERSISTENT),
            routing_key=routing_key,
        )
    finally:
        if chan is not None:
            try:
                await chan.close()
            except Exception:
                pass


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\core\__init__.py ===
"""Core helpers package for DB and MQ."""

__all__ = ["db", "mq"]


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\services\ai_analyzer.py ===
import asyncio
from typing import Dict
from app.services.pii_detector.scanner import PIIScanner

GLOBAL_SCANNER = PIIScanner()

async def analyze_text(text: str) -> Dict:
    pii_results = await asyncio.to_thread(GLOBAL_SCANNER.analyze_text, text)

    anonymized_result = await asyncio.to_thread(
        GLOBAL_SCANNER.anonymize_text, 
        text, 
        pii_results
    )

    detections = [
        {
            "type": res.entity_type,
            "score": round(res.score, 2),
            "start": res.start,
            "end": res.end,
            "value": text[res.start:res.end] 
        }
        for res in pii_results
    ]

    return {
        "summary": text[:100] + "..." if len(text) > 100 else text,
        "length": len(text),
        "pii_count": len(pii_results),
        "pii_detected": detections,
        "anonymized_text": anonymized_result.text
    }



=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\services\dispatcher_worker.py ===
import asyncio
import os
import json
import time
import aiohttp
import logging
import traceback
from app.core.db import init_db
from app.models import Status, ShipmentStatus, RegisterProcessStatus
from app.repository import (
    fetch_pending_shipments,
    mark_shipment_sent,
    update_register_process_status_by_processing_id,
    set_shipment_status_by_processing_id,
    get_shipment_attempts_by_processing_id,
    update_status,
)

WEBHOOK_URL = os.getenv("WEBHOOK_URL")
DISPATCH_INTERVAL = int(os.getenv("DISPATCH_INTERVAL", "30"))  # Intervalo em segundos entre verificações de envios pendentes

CB_FAILURE_THRESHOLD = int(os.getenv("CB_FAILURE_THRESHOLD", "3")) # Número de falhas para abrir o circuito
CB_RECOVERY_TIMEOUT = int(os.getenv("CB_RECOVERY_TIMEOUT", "60"))   # Tempo em segundos para tentar fechar o circuito novamente
CB_HALF_OPEN_MAX_CALLS = int(os.getenv("CB_HALF_OPEN_MAX_CALLS", "1")) # Número de chamadas permitidas no estado half-open


class CircuitBreaker:
    """Simple in-memory async-friendly circuit breaker.

    States:
      - CLOSED: allow requests, count failures
      - OPEN: block requests until recovery timeout
      - HALF_OPEN: allow a small number of trial requests
    """

    def __init__(self, failure_threshold=3, recovery_timeout=60, half_open_max_calls=1):
        self._failure_threshold = int(failure_threshold)
        self._recovery_timeout = int(recovery_timeout)
        self._half_open_max_calls = int(half_open_max_calls)
        self._state = "CLOSED"
        self._failure_count = 0
        self._opened_at = None
        self._half_open_calls = 0

    def _now(self):
        return time.monotonic()

    def allow(self):
        if self._state == "OPEN":
            # still cooling down?
            if (self._now() - self._opened_at) >= self._recovery_timeout:
                # move to half-open and allow trial
                self._state = "HALF_OPEN"
                self._half_open_calls = 0
                return True
            return False
        if self._state == "HALF_OPEN":
            if self._half_open_calls < self._half_open_max_calls:
                self._half_open_calls += 1
                return True
            return False
        # CLOSED
        return True

    def record_success(self):
        self._failure_count = 0
        self._state = "CLOSED"
        self._opened_at = None
        self._half_open_calls = 0

    def record_failure(self):
        self._failure_count += 1
        if self._failure_count >= self._failure_threshold:
            self._state = "OPEN"
            self._opened_at = self._now()

    @property
    def state(self):
        return self._state

async def dispatch_pending():
    await init_db()
    cb = CircuitBreaker(
        failure_threshold=CB_FAILURE_THRESHOLD,
        recovery_timeout=CB_RECOVERY_TIMEOUT,
        half_open_max_calls=CB_HALF_OPEN_MAX_CALLS,
    )

    while True:
        rows = await fetch_pending_shipments()
        if not rows:
            await asyncio.sleep(DISPATCH_INTERVAL)
            continue

        for row in rows:
            proc_id = row["processing_id"]
            result = row["result"]
            shipment = row["shipment"]

            if not WEBHOOK_URL:
                logging.warning("WEBHOOK_URL not configured; skipping dispatch")
                break

            if not cb.allow():
                logging.info(f"Circuit open; skipping dispatch for id={proc_id}")
                continue

            # mark processing as on_shipment
            try:
                await update_status(proc_id, Status.ON_SHIPMENT.value)
            except Exception:
                logging.exception(f"Failed to mark processing {proc_id} as ON_SHIPMENT")

            # prepare payload ensuring UUIDs are converted to strings
            payload = {"id": str(proc_id), "result": result, "shipment": shipment}
            try:
                payload_json = json.dumps(payload, default=str)
            except Exception as ser_e:
                logging.exception(f"Failed to JSON-serialize payload for id={proc_id}: {ser_e}")
                # persist retryable error and continue
                try:
                    await set_shipment_status_by_processing_id(proc_id, ShipmentStatus.ERROR_RETRY.value, increment_attempt=True)
                    await update_status(proc_id, Status.ERROR.value)
                    await update_register_process_status_by_processing_id(proc_id, RegisterProcessStatus.ERROR_RETRY.value)
                except Exception:
                    logging.exception(f"Failed to persist serialization error state for id={proc_id}")
                cb.record_failure()
                continue

            try:
                async with aiohttp.ClientSession() as session:
                    headers = {"Content-Type": "application/json"}
                    resp = await session.post(WEBHOOK_URL, data=payload_json, headers=headers)
                    resp_text = await resp.text()
                    if 200 <= resp.status < 300:
                        await mark_shipment_sent(proc_id)
                        # finalize processing
                        await update_status(proc_id, Status.FINISHED.value)
                        await update_register_process_status_by_processing_id(proc_id, RegisterProcessStatus.COMPLETED.value)
                        cb.record_success()
                        logging.info(f"Dispatched shipment for id={proc_id}: status={resp.status}")
                    else:
                        # non-2xx responses count as failures
                        cb.record_failure()
                        logging.error(f"Dispatch failed for id={proc_id}: status={resp.status}, body={resp_text}, CB={cb.state}")
                        # 5xx -> retryable
                        if resp.status >= 500:
                            try:
                                await set_shipment_status_by_processing_id(proc_id, ShipmentStatus.ERROR_RETRY.value, increment_attempt=True)
                                await update_status(proc_id, Status.ERROR.value)
                                await update_register_process_status_by_processing_id(proc_id, RegisterProcessStatus.ERROR_RETRY.value)
                            except Exception:
                                logging.exception(f"Failed to persist retryable error state for id={proc_id}")
                        else:
                            # 4xx -> fatal
                            try:
                                await set_shipment_status_by_processing_id(proc_id, ShipmentStatus.ERROR_FATAL.value, increment_attempt=False)
                                await update_status(proc_id, Status.ERROR.value)
                                await update_register_process_status_by_processing_id(proc_id, RegisterProcessStatus.ERROR_FATAL.value)
                            except Exception:
                                logging.exception(f"Failed to persist fatal error state for id={proc_id}")
            except Exception as e:
                cb.record_failure()
                logging.exception(f"Exception while dispatching id={proc_id}: {e} (CB state={cb.state})")
                # network/exception -> treat as retryable
                try:
                    await set_shipment_status_by_processing_id(proc_id, ShipmentStatus.ERROR_RETRY.value, increment_attempt=True)
                    await update_status(proc_id, Status.ERROR.value)
                    await update_register_process_status_by_processing_id(proc_id, RegisterProcessStatus.ERROR_RETRY.value)
                except Exception:
                    logging.exception(f"Failed to persist error state for id={proc_id}")

        await asyncio.sleep(DISPATCH_INTERVAL)


if __name__ == "__main__":
    asyncio.run(dispatch_pending())

=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\services\worker.py ===
import asyncio
import json
import signal
from venv import logger
from aio_pika import IncomingMessage
import asyncpg
from app.core.db import init_db
from app.models import Status, RegisterProcessStatus
from app.services.ai_analyzer import analyze_text
from app.core import mq
from app.repository import update_status, update_result, update_register_process_status_by_processing_id


async def handle_message(message: IncomingMessage):
    # use explicit ack/nack handling so we can requeue when Postgres is overloaded
    async with message.process(): # Gerencia ACK/NACK automaticamente
        try:
            body = json.loads(message.body.decode()) 
            proc_id = body.get("id")
            text = body.get("originalText") 

            # 1. Marcar como processando
            await update_status(proc_id, Status.PROCESSING.value)

            # 2. IA - Processamento intensivo fora do loop de IO se necessário
            result = await analyze_text(text) 

            # 3. Salvar resultado e finalizar
            await update_result(proc_id, result)
            
            logger.info(f"Processed successfully: {proc_id}")

        except Exception as e:
            logger.error(f"Error processing {proc_id}: {e}")

async def run_worker():
    await init_db()
    connection, channel, queue = await mq.init_rabbit()

    # Setup graceful shutdown handling
    stop_event = asyncio.Event()
    loop = asyncio.get_running_loop()

    def _set_stop():
        stop_event.set()

    try:
        for s in (signal.SIGINT, signal.SIGTERM):
            try:
                loop.add_signal_handler(s, _set_stop)
            except NotImplementedError:
                # add_signal_handler may not be implemented on Windows event loop
                pass
    except Exception:
        pass

    await queue.consume(handle_message)
    print("Worker started, waiting for messages...")
    try:
        await stop_event.wait()
    finally:
        # close the connection cleanly on the same loop
        await mq.close_rabbit(connection)


if __name__ == "__main__":
    asyncio.run(run_worker())


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\services\__init__.py ===
"""Services package (AI, workers, etc)"""


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\services\pii_detector\custom_patterns.py ===
from presidio_analyzer import Pattern, PatternRecognizer
from validate_docbr import CNPJ, CPF, CNS, CNH, PIS, RENAVAM, TituloEleitoral

class CnpjRecognizer(PatternRecognizer):
    def __init__(self):
        self.cnpj_validator = CNPJ()
        patterns = [
            Pattern(
                name="cnpj_pattern",
                regex=r"\b\d{2}[\s.]?\d{3}[\s.]?\d{3}[\s/]?\d{4}[\s-]?\d{2}\b",
                score=0.85
            )
        ]
        
        super().__init__(
            supported_entity="CNPJ",
            patterns=patterns,
            context=["cnpj", "documento", "cadastro"],
            supported_language="pt"
        )

    def validate_result(self, pattern_text: str) -> bool:
        return self.cnpj_validator.validate(pattern_text)

class CpfRecognizer(PatternRecognizer):
    def __init__(self):
        self.cpf_validator = CPF()

        patterns = [
            Pattern(
                name="cpf_pattern",
                regex=r"\b\d{3}[\s.-]?\d{3}[\s.-]?\d{3}[\s.-]?\d{2}\b",
                score=0.85
            )
        ]
        
        super().__init__(
            supported_entity="CPF",
            patterns=patterns,
            context=["cpf", "documento", "cadastro"],
            supported_language="pt"
        )

    def validate_result(self, pattern_text: str) -> bool:
        result = self.cpf_validator.validate(pattern_text)
        print(f'pattern_text + {result}')
        return self.cpf_validator.validate(pattern_text)

class CnsRecognizer(PatternRecognizer):
    def __init__(self):
        self.validator = CNS()

        patterns = [
            Pattern(
                name="cns_pattern",
                regex=r"\b\d{3}[\s.]?\d{4}[\s.]?\d{4}[\s.]?\d{4}\b",
                score=0.6
            )
        ]

        super().__init__(
            supported_entity="CNS",
            patterns=patterns,
            context=["sus", "saúde", "cartão", "médico", "paciente", "cns"],
            supported_language="pt"
        )

    def validate_result(self, pattern_text: str) -> bool:
        return self.validator.validate(pattern_text)

class CnhRecognizer(PatternRecognizer):
    def __init__(self):
        self.validator = CNH()

        patterns = [
            Pattern(
                name="cnh_pattern",
                regex=r"\b\d{11}\b",
                score=0.4
            )
        ]

        super().__init__(
            supported_entity="CNH",
            patterns=patterns,
            context=["cnh", "habilitação", "motorista", "condutor", "detran", "carteira"],
            supported_language="pt"
        )

    def validate_result(self, pattern_text: str) -> bool:
        return self.validator.validate(pattern_text)

class CarPlateRecognizer(PatternRecognizer):
    def __init__(self):
        patterns = [
            Pattern(
                name="plate_pattern",
                regex=r"\b[A-Z]{3}-?\d[\dA-J]\d{2}\b",
                score=0.85
            )
        ]
        
        super().__init__(
            supported_entity="CAR_PLATE",
            patterns=patterns,
            context=["placa", "veículo", "carro", "moto", "detran", "multa"],
            supported_language="pt"
        )

class PisRecognizer(PatternRecognizer):
    def __init__(self):
        self.validator = PIS()

        patterns = [
            Pattern(
                name="pis_pattern",
                regex=r"\b\d{3}[\s.]?\d{5}[\s.]?\d{2}[\s.-]?\d\b",
                score=0.6
            )
        ]

        super().__init__(
            supported_entity="PIS",
            patterns=patterns,
            context=["pis", "pasep", "nis", "nit", "trabalho", "previdência", "social"],
            supported_language="pt"
        )

    def validate_result(self, pattern_text: str) -> bool:
        return self.validator.validate(pattern_text)

class RenavamRecognizer(PatternRecognizer):
    def __init__(self):
        self.validator = RENAVAM()

        patterns = [
            Pattern(
                name="renavam_pattern",
                regex=r"\b\d{9,11}\b",
                score=0.4
            )
        ]

        super().__init__(
            supported_entity="RENAVAM",
            patterns=patterns,
            context=["renavam", "veículo", "carro", "moto", "detran", "ipva", "licenciamento"],
            supported_language="pt"
        )

    def validate_result(self, pattern_text: str) -> bool:
        return self.validator.validate(pattern_text)

class TituloEleitoralRecognizer(PatternRecognizer):
    def __init__(self):
        self.validator = TituloEleitoral()

        patterns = [
            Pattern(
                name="titulo_eleitor_pattern",
                regex=r"\b\d{4}[\s.]?\d{4}[\s.]?\d{4}\b",
                score=0.6
            )
        ]

        super().__init__(
            supported_entity="TITULO_ELEITORAL",
            patterns=patterns,
            context=["título", "eleitor", "votação", "zona", "seção", "eleitoral"],
            supported_language="pt"
        )

    def validate_result(self, pattern_text: str) -> bool:
        return self.validator.validate(pattern_text)

# por enquanto n consegui fazer funcionar muito bem :(
# TO-DO: refazer PasswordRecognizer
class PasswordRecognizer(PatternRecognizer):
    def __init__(self):
        patterns = [
            Pattern(
                name="password_pattern", 
                regex=r"(?i)\b(?:senha|password|passwd|pin|secret|token)\b\s*(?:[:=]|\b(?:é|is)\b)?\s*([^\s.,]+)",
                score=1)
        ]
        
        super().__init__(
            supported_entity="SENHA",
            patterns=patterns,
            supported_language="pt"
        )

class CustomEmailRecognizer(PatternRecognizer):
    def __init__(self):
        patterns = [
            Pattern(
                name="email_pattern",
                regex=r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
                score=1
            ),
        ]
        
        super().__init__(
            supported_entity="EMAIL",
            patterns=patterns,
            supported_language="pt",
            name="CustomEmailRecognizer"
        )


RECOGNIZERS = [
    CpfRecognizer(),
    CnpjRecognizer(),
    CnhRecognizer(),
    TituloEleitoralRecognizer(),
    PisRecognizer(),
    RenavamRecognizer(),
    CnsRecognizer(),
    PasswordRecognizer(),
    CustomEmailRecognizer()
]


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\services\pii_detector\scanner.py ===
from presidio_analyzer import AnalyzerEngine, RecognizerRegistry
from presidio_analyzer.nlp_engine import NlpEngineProvider
from presidio_anonymizer import AnonymizerEngine
from app.services.pii_detector.settings import SPACY_CONFIG
from app.services.pii_detector.custom_patterns import RECOGNIZERS

class PIIScanner:

    def __init__(self):

        provider = NlpEngineProvider(nlp_configuration=SPACY_CONFIG)
        nlp_engine = provider.create_engine()

        registry = RecognizerRegistry()
        registry.load_predefined_recognizers()
        
        for recognizer in RECOGNIZERS:
            registry.add_recognizer(recognizer)

        self.analyzer = AnalyzerEngine(registry=registry, nlp_engine=nlp_engine)
        self.anonymizer = AnonymizerEngine()

    def analyze_text(self, text):
        """Retorna a lista de entidades encontradas"""
        return self.analyzer.analyze(text=text, language='pt')

    def anonymize_text(self, text, results):
        """Retorna o texto anonimizado"""
        return self.anonymizer.anonymize(text=text, analyzer_results=results)

=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\services\pii_detector\settings.py ===
SPACY_CONFIG = {
    "nlp_engine_name": "spacy",
    "models": [
        {"lang_code": "pt", "model_name": "pt_core_news_lg"},
        {"lang_code": "en", "model_name": "en_core_web_lg"}
    ],
}

=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\app\services\pii_detector\__init__.py ===


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\tests\__init__.py ===


=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\tests\services\test_ai_analyzer.py ===
import pytest
from app.services.ai_analyzer import analyze_text

@pytest.mark.asyncio
async def test_analise_assincrona_completa():
    texto_teste = "email danidani@gmaill.com e senha 0553193@!, mas a senha: 3213"


    result = await analyze_text(texto_teste)

    assert "summary" in result
    assert "pii_detected" in result
    assert "anonymized_text" in result

    assert result["pii_count"] > 0 # pra ver se ele detectou 
    
    final_text = result["anonymized_text"]

    print(result["anonymized_text"])

    assert "danidani@gmaill.com" not in final_text
    assert "066.319.351-64" not in final_text
    assert "*****" in final_text or "<EMAIL>" in final_text

=== C:\Users\skytl\CODE\projects\ParticipaDF\AcessoInformacao\api\tests\services\__init__.py ===

